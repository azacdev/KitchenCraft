import { eventSourceToAsyncIterable } from "@/lib/event-source";
import { sanitizeOutput } from "@/lib/llm";
import { assert } from "@/lib/utils";
import { Ollama } from "langchain/llms/ollama";
import { PromptTemplate } from "langchain/prompts";
import Replicate from "replicate";

type Input = {
  badYaml: string;
  error: string;
};

export const YamlFixer = async ({
  input,
  onError,
  onComplete,
}: {
  input: Input;
  onError?: (error: Error, outputRaw: string) => void;
  onComplete?: (outputYaml: any) => void;
}) => {
  const stream = await getYamlFixerStream({ input });
  const charArray: string[] = [];
  for await (const chunk of stream) {
    for (let char of chunk) {
      charArray.push(char);
    }
  }

  const outputRaw = charArray.join("");
  const outputYaml = sanitizeOutput(outputRaw);
  console.log({ outputRaw, outputYaml });

  onComplete && onComplete(outputYaml);

  return <></>;
};

async function getYamlFixerStream(props: { input: Input }) {
  if (process.env.NODE_ENV === "production") {
    return getReplicateStream(props);
  } else {
    return getOllamaStream(props);
  }
}

async function getOllamaStream({ input }: { input: Input }) {
  const llm = new Ollama({
    baseUrl: "http://localhost:11434",
    model: "mistral-openorca",
  });

  const promptTemplate = PromptTemplate.fromTemplate(
    FIX_YAML_TEMPLATE(input.error)
  );
  const chain = promptTemplate.pipe(llm);
  const stream = await chain.stream({
    prompt: input.badYaml,
  });

  return stream as AsyncIterable<string>;
}

const replicate = new Replicate();
async function getReplicateStream({ input }: { input: Input }) {
  try {
    const response = await replicate.predictions.create({
      version:
        "7afe21847d582f7811327c903433e29334c31fe861a7cf23c62882b181bacb88",
      stream: true,
      input: {
        temperature: 0.2,
        max_new_tokens: 512,
        prompt_template: FIX_YAML_TEMPLATE(input.error),
        prompt: input.badYaml,
      },
    });
    const { stream } = response.urls;
    assert(stream, "expected streamUrl");

    return eventSourceToAsyncIterable(stream);
  } catch (ex) {
    console.warn(ex);
    throw ex;
  }
}

const FIX_YAML_TEMPLATE = (error: string) => `<|im_start|>system:
The user will provide a yaml block that was generated by an LLM but failed to parse.

Here is the error received when parsing it ${error}.

Please fix the provided yaml so that it will parse.
<|im_end|>
<|im_start|>user:
\`\`\`yaml
{prompt}
\`\`\`
<|im_end|>
<|im_start|>assistant:
\`\`\`yaml
`;
